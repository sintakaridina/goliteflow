# GoliteFlow Examples

This directory contains real-world examples that demonstrate GoliteFlow's capabilities. Each example is fully functional and can be run immediately to see actual results and HTML reports.

## ğŸ“ Available Examples

### 1. **File Processing Workflow** (`file-processing-workflow.yml`)

A complete data processing pipeline that:

- âœ… Generates sample e-commerce data (CSV)
- âœ… Validates data quality and structure
- âœ… Processes data with analytics (profit, rankings, categorization)
- âœ… Generates comprehensive summary report
- âœ… Cleans up temporary files and archives results

### 2. **API Monitoring Workflow** (`api-monitoring-workflow.yml`)

Real-time API health monitoring that:

- âœ… Makes HTTP requests to test APIs
- âœ… Validates JSON response structure
- âœ… Logs API status and response times
- âœ… Tracks uptime statistics
- âœ… Generates alerts for failures

### 3. **Backup & Cleanup Workflow** (`backup-cleanup-workflow.yml`)

Automated maintenance operations:

- âœ… Creates data backups
- âœ… Compresses and verifies backups
- âœ… Cleans up old files
- âœ… Monitors disk usage
- âœ… Optimizes system resources

## ğŸš€ Quick Start

### Prerequisites

- **GoliteFlow binary** installed (see main README)
- **Python 3.6+** for support scripts
- **curl** for API monitoring examples

### Run Your First Example

```bash
# 1. Navigate to examples directory
cd examples/

# 2. Run the file processing workflow
goliteflow run --config=file-processing-workflow.yml

# 3. Generate HTML report
goliteflow report --output=processing_report.html

# 4. View results
open processing_report.html  # macOS
# or
start processing_report.html  # Windows
# or
xdg-open processing_report.html  # Linux
```

## ğŸ“Š Expected Results

### File Processing Example

After running, you'll see:

```
data/
â”œâ”€â”€ sample_data.csv          # 100 sample e-commerce records
â”œâ”€â”€ processed_data.csv       # Enhanced data with analytics
â””â”€â”€ summary_report.txt       # Business intelligence report

outputs/
â””â”€â”€ run_20231015_143022/     # Timestamped archive
    â”œâ”€â”€ processed_data.csv
    â”œâ”€â”€ summary_report.txt
    â””â”€â”€ cleanup_summary.txt
```

### Sample Output from Summary Report:

```
ğŸ“Š OVERALL STATISTICS
----------------------------------------
Total Orders: 100
Total Revenue: $52,847.32
Total Profit: $15,854.20
Average Order Value: $528.47
Average Profit Margin: 30.0%

ğŸ† TOP 5 PRODUCTS BY REVENUE
----------------------------------------
1. Laptop          $15,234.56 ( 23 units sold)
2. Monitor          $12,487.90 ( 19 units sold)
3. Phone            $10,928.34 ( 31 units sold)
```

### API Monitoring Example

Continuous monitoring with logs:

```
logs/
â”œâ”€â”€ api_response.json        # Latest API response
â”œâ”€â”€ api_validation.log       # Validation history
â”œâ”€â”€ api_status.log          # Status timeline
â””â”€â”€ api_status_detail.json  # Detailed metrics
```

## ğŸ¯ Example Workflows in Detail

### File Processing Pipeline

This example demonstrates a typical **ETL (Extract, Transform, Load)** workflow:

1. **Extract**: Generate sample sales data
2. **Transform**: Add calculated fields (profit, rankings, categories)
3. **Load**: Create reports and archive results

**Business Value:**

- Data quality validation
- Automated analytics
- Business intelligence reporting
- Clean data archival

### API Health Monitoring

Real-world **DevOps monitoring** scenario:

1. **Health Checks**: HTTP requests to API endpoints
2. **Validation**: JSON structure and data quality
3. **Alerting**: Status logging and failure detection
4. **Reporting**: Uptime statistics and trends

**Business Value:**

- Early problem detection
- SLA monitoring
- Performance tracking
- Incident response

### Backup & Maintenance

Enterprise **data protection** workflow:

1. **Backup Creation**: Copy critical data files
2. **Compression**: Optimize storage space
3. **Verification**: Ensure backup integrity
4. **Cleanup**: Remove old files and optimize resources

**Business Value:**

- Data protection
- Storage optimization
- Automated maintenance
- Compliance requirements

## ğŸ”§ Customizing Examples

### Modify Schedules

Change cron expressions in workflow files:

```yaml
schedule: "0 9 * * *"     # Daily at 9 AM
schedule: "*/15 * * * *"  # Every 15 minutes
schedule: "0 2 * * 0"     # Weekly on Sunday at 2 AM
```

### Add Your Own Tasks

Extend workflows with custom commands:

```yaml
- id: custom_task
  depends_on: ["previous_task"]
  command: "python my_custom_script.py"
  retry: 3
  timeout: "60s"
```

### Environment Variables

Configure scripts with environment variables:

```yaml
- id: upload_to_s3
  command: "aws s3 cp backup.zip s3://my-bucket/"
  env:
    AWS_ACCESS_KEY_ID: "your-key"
    AWS_SECRET_ACCESS_KEY: "your-secret"
```

## ğŸ“ˆ Viewing Results

### HTML Reports

GoliteFlow generates beautiful HTML reports showing:

- âœ… Task execution timeline
- âœ… Success/failure status
- âœ… Execution logs and output
- âœ… Performance metrics
- âœ… Retry attempts and errors

### Log Files

Each example generates structured logs:

- **Execution logs**: Task output and errors
- **Status logs**: Timeline of operations
- **Validation logs**: Data quality checks
- **Performance logs**: Timing and metrics

## ğŸ›ï¸ Running Examples

### One-Time Execution

```bash
# Run once and exit
goliteflow run --config=file-processing-workflow.yml
```

### Daemon Mode (Scheduled)

```bash
# Run as daemon with cron scheduling
goliteflow run --config=api-monitoring-workflow.yml --daemon
```

### Validate Before Running

```bash
# Check configuration syntax
goliteflow validate --config=backup-cleanup-workflow.yml
```

### Custom Report Output

```bash
# Generate report with custom filename
goliteflow report --output=my_custom_report.html
```

## ğŸ” Troubleshooting

### Common Issues

**Python not found:**

```bash
# Make sure Python is in PATH
python --version
python3 --version
```

**Permission errors:**

```bash
# Make scripts executable (Linux/macOS)
chmod +x scripts/*.py
```

**Missing directories:**
The scripts automatically create required directories, but you can create them manually:

```bash
mkdir -p data logs outputs backups
```

### Debug Mode

Add verbose logging to see detailed execution:

```bash
goliteflow run --config=workflow.yml --verbose
```

## ğŸ’¡ Next Steps

1. **Start with file processing** - Easy to understand and see results
2. **Try API monitoring** - See real-time monitoring in action
3. **Customize for your needs** - Modify scripts and schedules
4. **Build your own workflows** - Use examples as templates
5. **Share your examples** - Contribute back to the community

## ğŸ¤ Contributing Examples

Have a great workflow example? We'd love to include it! Please:

1. Create a complete, working example
2. Include supporting scripts and documentation
3. Test thoroughly on multiple platforms
4. Submit a pull request with:
   - Workflow YAML file
   - Supporting Python scripts
   - Sample data (if needed)
   - Documentation updates

---

**Happy workflow automation with GoliteFlow!** ğŸš€

For more information, see the main [GoliteFlow documentation](../README.md).
